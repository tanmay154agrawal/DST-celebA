# -*- coding: utf-8 -*-
"""DL_ASS3_Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17u3GO2oSu-ZZ84wdiAL2v5170JggDaBJ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models

import os
from PIL import Image
import pandas as pd
import imageio
from tqdm import tqdm
import numpy as np
import random

transform = transforms.Compose([
    transforms.CenterCrop(178),
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

print(os.getcwd())

!cp  /content/drive/MyDrive/CelebA/Img/img_align_celeba.zip /content/

!unzip "/content/drive/MyDrive/CelebA/Img/img_align_celeba.zip" -d "/content/celeba"

train_dataset = datasets.CelebA(root='.', split='train', transform=transform, target_type='attr', download=False)
test_dataset = datasets.CelebA(root='.', split='test', transform=transform, target_type='attr', download=False)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,drop_last=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False,drop_last=True)

# Load the pre-trained VGG16 model
vgg16 = models.vgg16(pretrained=True)

# Freeze the weights of the convolutional layers
for param in vgg16.features.parameters():
    param.requires_grad = False

# Replace the last fully connected layer with a new one that has 8 output neurons
num_features = vgg16.classifier[6].in_features
vgg16.classifier[6] = nn.Linear(num_features, 8)

# Define loss functions for each attribute
criterion1 = nn.CrossEntropyLoss()
criterion2 = nn.CrossEntropyLoss()
criterion3 = nn.CrossEntropyLoss()
criterion4 = nn.CrossEntropyLoss()
criterion5 = nn.CrossEntropyLoss()
criterion6 = nn.CrossEntropyLoss()
criterion7 = nn.CrossEntropyLoss()
criterion8 = nn.CrossEntropyLoss()

def multi_task_loss(y_pred, y_true):
    loss1 = criterion1(y_pred[0,:], y_true[0,:])
    loss2 = criterion2(y_pred[1,:], y_true[1,:])
    loss3 = criterion3(y_pred[2,:], y_true[2,:])
    loss4 = criterion4(y_pred[3,:], y_true[3,:])
    loss5 = criterion5(y_pred[4,:], y_true[4:,])
    loss6 = criterion6(y_pred[5,:], y_true[5:,])
    loss7 = criterion7(y_pred[6,:], y_true[6,:])
    loss8 = criterion8(y_pred[7,:], y_true[7,:])
    return loss1 + loss2 + loss3 + loss4 + loss5 + loss6 + loss7 + loss8

sigmoid=nn.Sigmoid()
class MultiTaskModel(nn.Module):
    def __init__(self, vgg):
        super(MultiTaskModel, self).__init__()
        self.features = vgg.features
        self.classifier = vgg.classifier

    def forward(self, x):
        # print(x.shape)
        x = self.features(x)
        # print(x.shape)
        x = x.view(x.size(0), -1)
        # print(x.shape)
        x = self.classifier(x)
        # print(x.shape)
        x = sigmoid(x)
        # print(x.shape)
        return x

model = MultiTaskModel(vgg16).cuda()
optimizer = optim.Adam(model.parameters(), lr=0.001)

accumulation_steps = 4
num_epochs = 5
for epoch in range(num_epochs):
    running_loss = 0.0
    for i, data in tqdm(enumerate(train_loader, 0)):
       x,y=data
      #  print(y.shape)
       y=y[:,:8]
       y=y.reshape(8,-1)
       labels1, labels2, labels3, labels4, labels5, labels6, labels7, labels8 = y.cuda()
      #  print(labels1)
       optimizer.zero_grad()
       x=x.cuda()
       outputs1, outputs2, outputs3, outputs4, outputs5, outputs6, outputs7, outputs8 = model(x).reshape(8,-1)
       loss1 = criterion1(outputs1, labels1.float())
       loss2 = criterion2(outputs2, labels2.float())
       loss3 = criterion3(outputs3, labels3.float())
       loss4 = criterion4(outputs4, labels4.float())
       loss5 = criterion5(outputs5, labels5.float())
       loss6 = criterion6(outputs6, labels6.float())
       loss7 = criterion7(outputs7, labels7.float())
       loss8 = criterion8(outputs8, labels8.float())
       loss = loss1 + loss2 + loss3 + loss4 + loss5 + loss6 + loss7 + loss8
       loss.backward()
        # accumulate gradients
       if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
   # update the model parameters with the remaining gradients
    if i % accumulation_steps != 0:
     optimizer.step()
    running_loss += loss.item()
    print('[%d] loss: %.3f' %(epoch + 1, running_loss / len(train_loader)))
    running_loss = 0.0

model.eval() # set model to evaluation mode
task_names = ['task1', 'task2', 'task3', 'task4', 'task5', 'task6', 'task7', 'task8']
task_correct = [0] * len(task_names)
task_total = [0] * len(task_names)
total_correct = 0
total_samples = 0

with torch.no_grad(): # disable gradient computation
    for data in tqdm(test_loader):
        x,y=data
        y=y[:,:8]
        y=y.reshape(8,-1)
        labels1, labels2, labels3, labels4, labels5, labels6, labels7, labels8 = y.cuda()
        x=x.cuda()
        outputs1, outputs2, outputs3, outputs4, outputs5, outputs6, outputs7, outputs8 = model(x).reshape(8,-1)
        predicted1 = (outputs1>0.5)
        predicted2 = (outputs2>0.5)
        predicted3 = (outputs3>0.5)
        predicted4 = (outputs4>0.5)
        predicted5 = (outputs5>0.5)
        predicted6 = (outputs6>0.5)
        predicted7 = (outputs7>0.5)
        predicted8 = (outputs8>0.5)

        for i, task_name in enumerate(task_names):
            task_total[i] += 64
            task_correct[i] += (eval('predicted'+str(i+1)) == eval('labels'+str(i+1))).sum().item()
        total_samples += 64
        total_correct += ((predicted1 == labels1) & (predicted2 == labels2) & (predicted3 == labels3) & (predicted4 == labels4) & (predicted5 == labels5) & (predicted6 == labels6) & (predicted7 == labels7) & (predicted8 == labels8)).sum().item()

task_acc = [100 * task_correct[i] / task_total[i] for i in range(len(task_names))]
overall_acc = 100 * total_correct / total_samples

print('Task-wise accuracy:')
for i, task_name in enumerate(task_names):
    print(f'{task_name}: {task_acc[i]:.2f}%')
print(f'Overall accuracy: {overall_acc:.2f}%')

"""Training with DST"""

accumulation_steps = 4
num_epochs = 5
for epoch in range(num_epochs):
    running_loss = 0.0
    for i, data in tqdm(enumerate(train_loader, 0)):
       x,y=data
      #  print(y.shape)
       y=y[:,:8]
       y=y.reshape(8,-1)
       gt_counts=[0,0,0,0,0,0,0,0]
       for i in range(8):
          for j in range(64):
            if(y[i,j]==1):
               gt_counts[i]+=1
       sum_values = sum(gt_counts)
       normalized = [x / sum_values for x in gt_counts]
       weights = [int(random.random() < normalized[t]) for t in range(8)]
       labels1, labels2, labels3, labels4, labels5, labels6, labels7, labels8 = y.cuda()
      #  print(labels1)
       optimizer.zero_grad()
       x=x.cuda()
       outputs1, outputs2, outputs3, outputs4, outputs5, outputs6, outputs7, outputs8 = model(x).reshape(8,-1)
       loss1 = criterion1(outputs1, labels1.float())
       loss2 = criterion2(outputs2, labels2.float())
       loss3 = criterion3(outputs3, labels3.float())
       loss4 = criterion4(outputs4, labels4.float())
       loss5 = criterion5(outputs5, labels5.float())
       loss6 = criterion6(outputs6, labels6.float())
       loss7 = criterion7(outputs7, labels7.float())
       loss8 = criterion8(outputs8, labels8.float())
       loss = weights[0]*loss1 + weights[1]*loss2 + weights[2]*loss3 + weights[3]*loss4 + weights[4]*loss5 + weights[5]*loss6 + weights[6]*loss7 + weights[7]*loss8
       loss.backward()
        # accumulate gradients
       if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
    
   # update the model parameters with the remaining gradients
    if i % accumulation_steps != 0:
     optimizer.step()
    running_loss += loss.item()
    print('[%d] loss: %.3f' %(epoch + 1, running_loss / len(train_loader)))
    running_loss = 0.0



model.eval() # set model to evaluation mode
task_names = ['task1', 'task2', 'task3', 'task4', 'task5', 'task6', 'task7', 'task8']
task_correct = [0] * len(task_names)
task_total = [0] * len(task_names)
total_correct = 0
total_samples = 0

with torch.no_grad(): # disable gradient computation
    for data in tqdm(test_loader):
        x,y=data
        y=y[:,:8]
        y=y.reshape(8,-1)
        labels1, labels2, labels3, labels4, labels5, labels6, labels7, labels8 = y.cuda()
        x=x.cuda()
        outputs1, outputs2, outputs3, outputs4, outputs5, outputs6, outputs7, outputs8 = model(x).reshape(8,-1)
        predicted1 = (outputs1>0.5)
        predicted2 = (outputs2>0.5)
        predicted3 = (outputs3>0.5)
        predicted4 = (outputs4>0.5)
        predicted5 = (outputs5>0.5)
        predicted6 = (outputs6>0.5)
        predicted7 = (outputs7>0.5)
        predicted8 = (outputs8>0.5)

        for i, task_name in enumerate(task_names):
            task_total[i] += 64
            task_correct[i] += (eval('predicted'+str(i+1)) == eval('labels'+str(i+1))).sum().item()
        total_samples += 64
        total_correct += ((predicted1 == labels1) & (predicted2 == labels2) & (predicted3 == labels3) & (predicted4 == labels4) & (predicted5 == labels5) & (predicted6 == labels6) & (predicted7 == labels7) & (predicted8 == labels8)).sum().item()

task_acc = [100 * task_correct[i] / task_total[i] for i in range(len(task_names))]
overall_acc = 100 * total_correct / total_samples

print('Task-wise accuracy:')
for i, task_name in enumerate(task_names):
    print(f'{task_name}: {task_acc[i]:.2f}%')
print(f'Overall accuracy: {overall_acc:.2f}%')

